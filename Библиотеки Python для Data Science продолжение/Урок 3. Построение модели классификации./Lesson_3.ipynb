{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 3. Построение модели классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "micro f1 - гармоническое среднее. Применяется, когда количества объектов разных классов кратно отличаются. Подсчитывает общее количество истинных срабатываний, false negatives и false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro f1 - просто средний F1-Score. macro f1 - плохо работает с несбалансированными классами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted - macro показатель, но с учетом дисбаланса классов (за счет весовых коэффициентов для каждого класса)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе XGBoost лежит алгоритм градиентного бустинга деревьев решений. Обучение ансамбля проводится последовательно в отличие, например от бэггинга. На каждой итерации вычисляются отклонения предсказаний уже обученного ансамбля на обучающей выборке. Следующая модель, которая будет добавлена в ансамбль будет предсказывать эти отклонения. Таким образом, добавив предсказания нового дерева к предсказаниям обученного ансамбля мы можем уменьшить среднее отклонение модели, котрое является таргетом оптимизационной задачи. Новые деревья добавляются в ансамбль до тех пор, пока ошибка уменьшается, либо пока не выполняется одно из правил \"ранней остановки\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности модели $\\newline$\n",
    "XGBoost поддерживает все возможности таких библиотек как scikit-learn с возможностью добавлять регуляризацию. Поддержаны три главные формы градиетного бустинга:\n",
    "* Стандартный градиентный бустинг с возможностью изменения скорости обучения(learning rate).\n",
    "* Стохастический градиентный бустинг[8] с возможностью семплирования по строкам и колонкам датасета.\n",
    "* Регуляризованный градиентный бустинг[9] с L1 и L2 регуляризацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности алгоритма $\\newline$\n",
    "Реализация алгоритма была разработана для эффективности вычислительных ресурсов времени и памяти. Цель проекта заключалась в том, чтобы наилучшим образом использовать имеющиеся ресурсы для обучения модели. Некоторые ключевые функции реализации алгоритма включают:\n",
    "\n",
    "* Различные стратегии обработки пропущенных данных.\n",
    "* Блочная структура для поддержки распараллеливания обучения деревьев.\n",
    "* Продолжение обучения для дообучения на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширяет алгоритм градиентного бустинга, добавляя тип автоматического выбора объектов, а также фокусируясь на примерах бустинга с большими градиентами. Это может привести к резкому ускорению обучения и улучшению прогнозных показателей. Таким образом, LightGBM стала де-факто алгоритмом для соревнований по машинному обучению при работе с табличными данными для задач регрессионного и классификационного прогностического моделирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как таковой LightGBM — это проект с открытым исходным кодом, библиотека программного обеспечения и алгоритм машинного обучения. То есть проект очень похож на Extreme Gradient Boosting или XGBoost technique.\n",
    "\n",
    "LightGBM была описана Голинь К., и соавт. в статье 2017 года под названием «LightGBM: A Highly Efficient Gradient Boosting Decision Tree». Реализация вводит две ключевые идеи: GOSS и EFB.\n",
    "Градиентная односторонняя выборка (GOSS) является модификацией градиентного бустинга, который фокусирует внимание на тех учебных примерах, которые приводят к большему градиенту, в свою очередь, ускоряя обучение и уменьшая вычислительную сложность метода.\n",
    "\n",
    "С помощью GOSS мы исключаем значительную долю экземпляров данных с небольшими градиентами и используем только остальные экземпляры для оценки прироста информации. Мы доказываем, что, поскольку экземпляры данных с большими градиентами играют более важную роль в вычислении информационного выигрыша, GOSS может получить довольно точную оценку информационного выигрыша с гораздо меньшим размером данных. \n",
    "\n",
    "Exclusive Feature Bundling (объединение взаимоисключающих признаков), или EFB, — это подход объединения разрежённых (в основном нулевых) взаимоисключающих признаков, таких как категориальные переменные входных данных, закодированные унитарным кодированием. Таким образом, это тип автоматического подбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместе эти два изменения могут ускорить время обучения алгоритма до 20 раз. Таким образом, LightGBM можно рассматривать как деревья решений с градиентным бустингом (GBDT) с добавлением GOSS и EFB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost — открытая программная библиотека, разработанная компанией Яндекс и реализующая уникальный патентованный алгоритм построения моделей машинного обучения, использующий одну из оригинальных схем градиентного бустинга. Основное API для работы с библиотекой реализовано для языка Python, также существует реализация для языка программирования R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost написан на C++, это библиотека для градиентного бустинга на деревьях решений. В ней поддержано несколько видов деревьев, в том числе так называемые «симметричные» деревья, которые используются в библиотеке по умолчанию. \n",
    "\n",
    "В чем профит oblivious-деревьев? Они быстро учатся, быстро применяются и помогают обучению быть более устойчивым к изменению параметров с точки зрения изменений итогового качества модели, что сильно уменьшает необходимость в подборе параметров. Библиотека — про то, чтобы было удобно использовать в продакшене, быстро учиться и сразу получать хорошее качество. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название CatBoost придумано в Яндексе. Это сокращение от categorical boosting, то есть бустинг с учётом категориальных признаков. Работа с категориальными признаками задается как главное преимущество модели. Может при потребности обрабатывать их своим способом, если категорий более чем указано в параметрах или же разбивать на dummie категории и обрабатывать. Также модель устойчива к переобучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из увиденного очень сильно проигрывает в скорости обучения, хотя почему-то позиционируется как самая быстрая модель. Плюсы, в нашем случае, оценить сложно, скорее используется для узкого круга задач. Много есть параметров настраиваемых в том числе автоматически, что облегчает работу с моделью. Но к плюсам это отнести сложно, только для начинающих или непонимающих наверное. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие реальные плюсы и минусы модели исходя из практики? В двух словах если!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
